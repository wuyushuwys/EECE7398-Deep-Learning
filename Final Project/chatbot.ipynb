{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('data/cornell_movie_dialogs_corpus/movie_lines.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('data/cornell_movie_dialogs_corpus/movie_conversations.txt', 'r', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
       " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
       " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
       " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
       " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n",
       " 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n",
       " \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n",
       " 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n",
       " 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n",
       " 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\",\n",
       " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in lines:\n",
    "    if len(line) > 0:\n",
    "        lineID, characterID, movieID, character, text = line.split(\" +++$+++ \")\n",
    "        id2line[lineID] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = [ ]\n",
    "for _conv in conv_lines:\n",
    "    if len(_conv) > 0:\n",
    "        character1, character2, movieID, conv = _conv.split(\" +++$+++ \")\n",
    "        convs.append(eval(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['L194', 'L195', 'L196', 'L197'],\n",
       " ['L198', 'L199'],\n",
       " ['L200', 'L201', 'L202', 'L203'],\n",
       " ['L204', 'L205', 'L206'],\n",
       " ['L207', 'L208'],\n",
       " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
       " ['L276', 'L277'],\n",
       " ['L280', 'L281'],\n",
       " ['L363', 'L364'],\n",
       " ['L365', 'L366']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "limit = 0\n",
    "for i in range(limit, limit+5):\n",
    "    print(questions[i])\n",
    "    print(answers[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(questions)==len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,*]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list(map(clean_text, questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = list(map(clean_text, answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, questions, answers, l=[2, 30], loading=False):\n",
    "        \"\"\"\n",
    "        questions: X seq list\n",
    "        answers: Y seq list\n",
    "        l: length range [min, max]\n",
    "        loading: loading word2index.csv or generate new one\n",
    "        \"\"\"\n",
    "        super(dataset, self)\n",
    "        questions = np.array(questions)\n",
    "        answers = np.array(answers)\n",
    "        self.min_length = l[0]\n",
    "        self.max_length = l[1]\n",
    "        questions, answers = self.length_filter(questions, answers)\n",
    "        questions, answers = self.length_filter(answers, questions)\n",
    "        self.word2index = {}\n",
    "        self.index2word = {}\n",
    "        self.load_vocab(questions, answers, loading, 5)\n",
    "        \n",
    "        self.questions, self.answers = self.sentences2tensors(questions, answers)\n",
    "#         self.questions = np.core.defchararray.add(questions, ' <EOS>')\n",
    "#         self.answers = np.core.defchararray.add(answers, ' <EOS>')\n",
    "        \n",
    "    def length_filter(self, data1, data2):\n",
    "        \"\"\"filter length based on data1\"\"\"\n",
    "        d1_len = np.array(list(map(lambda x: len(x.split()), data1)))\n",
    "        d1_idx = np.logical_and(d1_len<=self.max_length, d1_len>=self.min_length)\n",
    "        return data1[d1_idx], data2[d1_idx]\n",
    "    \n",
    "    def load_vocab(self, questions, answers, loading, threshold=5):\n",
    "        \"\"\"generate vocabulary\"\"\"\n",
    "        import csv, os, sys\n",
    "        if not loading:\n",
    "            words_count = {}\n",
    "            for question in questions:\n",
    "                for word in question.split():\n",
    "                    if word not in words_count:\n",
    "                        words_count[word] = 1\n",
    "                    else: words_count[word] += 1\n",
    "            for ans in answers:\n",
    "                for word in ans.split():\n",
    "                    if word not in words_count:\n",
    "                        words_count[word] = 1\n",
    "                    else: words_count[word] += 1\n",
    "            count = 0\n",
    "            for k,v in words_count.items():\n",
    "                if v >= threshold:\n",
    "                    count += 1\n",
    "            print(\"Size of total vocab:\", len(words_count))\n",
    "            print(\"Size of vocab will be used:\", count)\n",
    "            word2index = {'<PAD>':0,'<EOS>':1,'<UNK>':2,'<GO>':3}\n",
    "            word_idx = 4\n",
    "            for word, count in words_count.items():\n",
    "                if count >= threshold:\n",
    "                    word2index[word]=word_idx\n",
    "                    word_idx += 1\n",
    "\n",
    "            file_w2i = open(\"word2index.csv\", \"w\")        \n",
    "            writer = csv.writer(file_w2i)\n",
    "            for key, value in word2index.items():\n",
    "                writer.writerow([key, value])\n",
    "            file_w2i.close()\n",
    "        elif loading and not os.path.exists('word2index.csv'):\n",
    "            sys.stderr.write(\"\"*40)\n",
    "            sys.stderr.write(\"\\n\\nNo Such File Found\\n\")\n",
    "            sys.stderr.write(\"\\nMust be .csv file\\nFile must under current directory\\nFile must named word2index.csv\")\n",
    "        else:\n",
    "            print(\"Loading word2index.csv\")\n",
    "            with open(\"word2index.csv\", \"r\") as csvfile:\n",
    "                reader = csv.reader(csvfile, delimiter=',')\n",
    "                word2index = {word: eval(idx) for (word, idx) in reader}\n",
    "            print(\"Loading complete\")\n",
    "            \n",
    "        self.word2index = word2index\n",
    "        self.index2word = {idx : w for w, idx in word2index.items()}\n",
    "        print(\"Size of vocab:\", len(word2index))\n",
    "        \n",
    "    \n",
    "    def sentences2tensors(self, questions, answers):\n",
    "        questions = np.core.defchararray.add(questions, ' <EOS>').tolist()\n",
    "        answers = np.core.defchararray.add(answers, ' <EOS>').tolist()\n",
    "        \n",
    "        def sent2tensor(sentence):\n",
    "            words = sentence.split()\n",
    "            seq = np.zeros(self.max_length+1)\n",
    "            for i, word in enumerate(words):\n",
    "                if word not in self.word2index:\n",
    "                    seq[i] = self.word2index['<UNK>']\n",
    "                else:\n",
    "                    seq[i] = self.word2index[word]\n",
    "            return seq\n",
    "        \n",
    "        q_tensors = torch.tensor(list(map(sent2tensor, questions))).long()\n",
    "        a_tensors = torch.tensor(list(map(sent2tensor, answers))).long()\n",
    "        print(\"Total number of questions\", q_tensors.shape)\n",
    "        print(\"Total number of answers\", a_tensors.shape)\n",
    "        \n",
    "        num_not_padding = (q_tensors!=0).sum() + (a_tensors!=0).sum() - q_tensors.shape[0]*2\n",
    "        num_unk = (q_tensors==2).sum() + (a_tensors==2).sum()\n",
    "        print(\"Percent of words that are <UNK>: {:.02f}%\".format(num_unk.float()/num_not_padding.float()*100))\n",
    "\n",
    "        \n",
    "        return q_tensors, a_tensors\n",
    "            \n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        X = self.questions[i, :]\n",
    "        y = self.answers[i, :]\n",
    "        return X, y\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.questions.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2index.csv\n",
      "Loading complete\n",
      "Size of vocab: 16877\n",
      "Total number of questions torch.Size([165571, 31])\n",
      "Total number of answers torch.Size([165571, 31])\n",
      "Percent of words that are <UNK>: 2.18%\n"
     ]
    }
   ],
   "source": [
    "data = dataset(questions, answers, loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([14, 24, 25, 26,  7, 27, 28, 29, 30, 31, 32, 33,  1,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([16, 17, 18, 19, 20, 19, 21, 22, 23,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.utils.data.DataLoader(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
